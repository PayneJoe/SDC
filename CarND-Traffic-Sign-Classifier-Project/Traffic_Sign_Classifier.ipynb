{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-30T15:31:29.048871Z",
     "start_time": "2018-03-30T15:31:29.036079Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "import sys,os,time\n",
    "import tensorflow as tf\n",
    "from contextlib import contextmanager\n",
    "%matplotlib inline\n",
    "\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    \"\"\"\n",
    "    Taken from Konstantin Lopuhin https://www.kaggle.com/lopuhin\n",
    "    in script named : Mercari Golf: 0.3875 CV in 75 LOC, 1900 s\n",
    "    https://www.kaggle.com/lopuhin/mercari-golf-0-3875-cv-in-75-loc-1900-s\n",
    "    \"\"\"\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(f'[{name}] done in {time.time() - t0:.0f} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-30T15:31:29.187661Z",
     "start_time": "2018-03-30T15:31:29.050538Z"
    }
   },
   "outputs": [],
   "source": [
    "## Load pickled data\n",
    "training_file = 'train.p'\n",
    "validation_file= 'valid.p'\n",
    "testing_file = 'test.p'\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(validation_file, mode='rb') as f:\n",
    "    valid = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "X_train, y_train = train['features'], train['labels']\n",
    "X_valid, y_valid = valid['features'], valid['labels']\n",
    "X_test, y_test = test['features'], test['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-30T15:31:29.321965Z",
     "start_time": "2018-03-30T15:31:29.190370Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Propotion of classes in training examples: \n",
      "[ 0.00517256  0.05689819  0.05776028  0.03620794  0.05086353  0.04741516\n",
      "  0.01034512  0.03707003  0.03620794  0.03793212  0.05172562  0.03362166\n",
      "  0.05431191  0.055174    0.01982816  0.01551769  0.01034512  0.02844909\n",
      "  0.03103537  0.00517256  0.00862094  0.00775884  0.00948303  0.01293141\n",
      "  0.00689675  0.03879422  0.01551769  0.00603466  0.0137935   0.00689675\n",
      "  0.01120722  0.01982816  0.00603466  0.01721314  0.01034512  0.03103537\n",
      "  0.00948303  0.00517256  0.05344981  0.00775884  0.00862094  0.00603466\n",
      "  0.00603466]\n",
      "Number of training examples = 34799\n",
      "Number of validation examples = 4410\n",
      "Number of testing examples = 12630\n",
      "Image data shape = (32, 32, 3)\n",
      "Number of classes = 43\n"
     ]
    }
   ],
   "source": [
    "## basic info of data sets\n",
    "# Number of training examples\n",
    "n_train = X_train.shape[0]\n",
    "\n",
    "# Number of validation examples\n",
    "n_validation = X_valid.shape[0]\n",
    "\n",
    "# Number of testing examples.\n",
    "n_test = X_test.shape[0]\n",
    "\n",
    "# What's the shape of an traffic sign image?\n",
    "image_shape = X_train[0].shape\n",
    "\n",
    "# How many unique classes/labels there are in the dataset.\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "# What propotions do these classes/labels take?\n",
    "p_classes = dict((c, 0) for c in range(n_classes))\n",
    "for l in y_train:\n",
    "    p_classes[l] += 1\n",
    "print('Propotion of classes in training examples: ')\n",
    "print(np.divide(list(p_classes.values()),np.sum(list(p_classes.values()))))\n",
    "\n",
    "print(\"Number of training examples =\", n_train)\n",
    "print(\"Number of validation examples =\", n_validation)\n",
    "print(\"Number of testing examples =\", n_test)\n",
    "print(\"Image data shape =\", image_shape)\n",
    "print(\"Number of classes =\", n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-30T15:32:17.610242Z",
     "start_time": "2018-03-30T15:32:16.975882Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size label 21 is 270\n",
      "(34799, 32, 32, 3)\n",
      "(270, 32, 32, 3)\n",
      "(1, 32, 32, 3)\n",
      "0 done.\n",
      "(3, 32, 32, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuanpingzhou/miniconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2918: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "## visualization for image\n",
    "def plot(image):\n",
    "    plt.figure(figsize=(1,1))\n",
    "    plt.imshow(image, cmap=\"gray\")\n",
    "    plt.show()\n",
    "    \n",
    "## brightness for image\n",
    "def brightness(image):\n",
    "    \n",
    "    image = tf.image.random_hue(image, max_delta=0.05)\n",
    "    image = tf.image.random_contrast(image, lower=0.3, upper=1.0)\n",
    "    image = tf.image.random_brightness(image, max_delta=0.1)\n",
    "    image = tf.image.random_saturation(image, lower=0.0, upper=2.0)   \n",
    "    image = tf.cast(image, tf.uint8)\n",
    "    \n",
    "    return image\n",
    "\n",
    "def paintwhite1(image):\n",
    "\n",
    "    m1 = tf.cast(tf.less(image, tf.fill([32, 32, 3], np.uint8(65))), tf.uint8)\n",
    "    white_img = tf.fill([32, 32, 3], np.uint8(255))\n",
    "    \n",
    "    image = tf.maximum(tf.cast(image, tf.int32), tf.cast(tf.multiply(white_img, m1), tf.int32))\n",
    "    image = tf.cast(image, tf.uint8)\n",
    "    \n",
    "    return image\n",
    "\n",
    "def paintwhite2(image):\n",
    "\n",
    "    m1 = tf.cast(tf.less(image, tf.fill([32, 32, 3], np.uint8(120))), tf.uint8)\n",
    "    m2 = tf.cast(tf.greater(image, tf.fill([32, 32, 3], np.uint8(65))), tf.uint8)\n",
    "    m = tf.multiply(m1, m2)\n",
    "    white_img = tf.fill([32, 32, 3], np.uint8(255))\n",
    "    \n",
    "    image = tf.maximum(tf.cast(image, tf.int32), tf.cast(tf.multiply(white_img, m), tf.int32))\n",
    "    image = tf.cast(image, tf.uint8)\n",
    "    \n",
    "    return image\n",
    "\n",
    "## scale for image\n",
    "def scale(image):\n",
    "    \n",
    "    image = tf.divide(tf.cast(image, tf.float32), 255)\n",
    "    image = tf.random_crop(image, size=[24, 24, 3])\n",
    "    image = tf.image.resize_images(image, (32, 32))\n",
    "    image = tf.cast(tf.multiply(image, 255.0), tf.uint8)\n",
    "    \n",
    "    return image\n",
    "\n",
    "## augmentation for train images\n",
    "def augment_data(dataset, dataset_labels, augementation_factor= 5):\n",
    "    augmented_image = []\n",
    "    augmented_image_labels = []\n",
    "\n",
    "    img_input = tf.placeholder(tf.uint8, (32, 32, 3))\n",
    "    # random brightness\n",
    "    random_brightness = brightness(img_input)\n",
    "    # scale\n",
    "    scaling = scale(img_input)\n",
    "    # painting\n",
    "    painting1 = paintwhite1(img_input)\n",
    "    painting2 = paintwhite2(img_input)\n",
    "    \n",
    "    n = 0\n",
    "    with tf.Session() as sess:\n",
    "        for num in range (0, dataset.shape[0]):\n",
    "            # original image\n",
    "            augmented_image.append(dataset[num].tolist())\n",
    "            augmented_image_labels.append(dataset_labels[num])\n",
    "            for i in range(0, augementation_factor):\n",
    "                # brightness for image\n",
    "                result_img = sess.run(random_brightness, feed_dict= {img_input: dataset[num]})\n",
    "                augmented_image.append(result_img.tolist())\n",
    "                augmented_image_labels.append(dataset_labels[num])\n",
    "                # nosing for image\n",
    "                result_img = sess.run(scaling, feed_dict= {img_input: dataset[num]})\n",
    "                augmented_image.append(result_img.tolist())\n",
    "                augmented_image_labels.append(dataset_labels[num])\n",
    "            # painting for image\n",
    "            result_img = sess.run(painting1, feed_dict= {img_input: dataset[num]})\n",
    "            augmented_image.append(result_img.tolist())\n",
    "            augmented_image_labels.append(dataset_labels[num])\n",
    "            \n",
    "            result_img = sess.run(painting2, feed_dict= {img_input: dataset[num]})\n",
    "            augmented_image.append(result_img.tolist())\n",
    "            augmented_image_labels.append(dataset_labels[num])\n",
    "            if((n % 1000) == 0):\n",
    "                print('%s done.' % n)\n",
    "            n += 1\n",
    "    return np.array(augmented_image), np.array(augmented_image_labels)\n",
    "\n",
    "print('original shape: ')\n",
    "print(X_train.shape)\n",
    "with timer('Augmentation ')\n",
    "    X_train, y_train = augment_data(X_train, y_train)\n",
    "print('augmented shape: ')\n",
    "print(X_train.shape)\n",
    "\n",
    "# label = 21\n",
    "# idx = np.argwhere(y_train == label).flatten()\n",
    "# print('size label %s is %s' % (label, len(idx)))\n",
    "# print(X_train.shape)\n",
    "# X_train_1 = X_train[idx,]\n",
    "# y_train_1 = y_train[idx,]\n",
    "# print(X_train_1.shape)\n",
    "\n",
    "# tmp_x = X_train_1[:1,]\n",
    "# tmp_y = y_train_1[:1,]\n",
    "# print(tmp_x.shape)\n",
    "# tmp_x, tmp_y = augment_data(tmp_x, tmp_y)\n",
    "# print(tmp_x.shape)\n",
    "# for i in range(len(tmp_x)):\n",
    "#     plot(tmp_x[i])\n",
    "# sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-30T15:31:29.997035Z",
     "start_time": "2018-03-30T15:31:29.039Z"
    }
   },
   "outputs": [],
   "source": [
    "## visualization for image\n",
    "\n",
    "index = random.randint(0, len(X_train))\n",
    "image = X_train[index].squeeze()\n",
    "\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.imshow(image, cmap=\"gray\")\n",
    "print(y_train[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-30T15:31:29.998090Z",
     "start_time": "2018-03-30T15:31:29.040Z"
    }
   },
   "outputs": [],
   "source": [
    "## LE-Net architechure\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import flatten\n",
    "\n",
    "def LeNet(x):    \n",
    "    # Arguments used for tf.truncated_normal, randomly defines variables for the weights and biases for each layer\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    \n",
    "    # SOLUTION: Layer 1: Convolutional. Input = 32x32x3. \n",
    "    # out_height = (32 - 5 + 1)/1 = 28,\n",
    "    # out_width = (32 - 5 + 1)/1 = 28,\n",
    "    # out_depth = 6(tunned)\n",
    "    # Output = 28 * 28 * 6\n",
    "    conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 3, 6), mean = mu, stddev = sigma))\n",
    "    conv1_b = tf.Variable(tf.zeros(6))\n",
    "    conv1   = tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b\n",
    "\n",
    "    # SOLUTION: Activation.\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "\n",
    "    # SOLUTION: Pooling. Input = 28x28x6. \n",
    "    # out_height = (28 - 2 + 2)/2 = 14,\n",
    "    # out_width = (28 - 2 + 2)/2 = 14,\n",
    "    # out_depth = 6(handed by conv1)\n",
    "    # Output = 14x14x6.\n",
    "    conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    # SOLUTION: Layer 2: Convolutional. \n",
    "    # out_height = (14 - 5 + 1)/1 = 10,\n",
    "    # out_width = (14 - 5 + 1)/1 = 10,\n",
    "    # out_depth = 16(tunned)\n",
    "    # Output = 10x10x16.\n",
    "    conv2_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 6, 16), mean = mu, stddev = sigma))\n",
    "    conv2_b = tf.Variable(tf.zeros(16))\n",
    "    conv2   = tf.nn.conv2d(conv1, conv2_W, strides=[1, 1, 1, 1], padding='VALID') + conv2_b\n",
    "    \n",
    "    # SOLUTION: Activation.\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "\n",
    "    # SOLUTION: Pooling. Input = 10x10x16. \n",
    "    # out_height = (10 - 2 + 2)/2 = 5,\n",
    "    # out_width = (10 - 2 + 2)/2 = 5,\n",
    "    # out_depth = 16(handed by conv2)\n",
    "    # Output = 5x5x16.\n",
    "    conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    # SOLUTION: Flatten. Input = 5x5x16.\n",
    "    # Output = 5 * 5 * 16 = 400.\n",
    "    fc0   = flatten(conv2)\n",
    "    \n",
    "    # SOLUTION: Layer 3: Fully Connected. Input = 400. Output = 120.\n",
    "    fc1_W = tf.Variable(tf.truncated_normal(shape=(400, 120), mean = mu, stddev = sigma))\n",
    "    fc1_b = tf.Variable(tf.zeros(120))\n",
    "    fc1   = tf.matmul(fc0, fc1_W) + fc1_b\n",
    "    \n",
    "    # SOLUTION: Activation.\n",
    "    fc1    = tf.nn.relu(fc1)\n",
    "\n",
    "    # SOLUTION: Layer 4: Fully Connected. Input = 120. Output = 84.\n",
    "    fc2_W  = tf.Variable(tf.truncated_normal(shape=(120, 84), mean = mu, stddev = sigma))\n",
    "    fc2_b  = tf.Variable(tf.zeros(84))\n",
    "    fc2    = tf.matmul(fc1, fc2_W) + fc2_b\n",
    "    \n",
    "    # SOLUTION: Activation.\n",
    "    fc2    = tf.nn.relu(fc2)\n",
    "\n",
    "    # SOLUTION: Layer 5: Fully Connected. Input = 84. Output = 43.\n",
    "    fc3_W  = tf.Variable(tf.truncated_normal(shape=(84, 43), mean = mu, stddev = sigma))\n",
    "    fc3_b  = tf.Variable(tf.zeros(43))\n",
    "    logits = tf.matmul(fc2, fc3_W) + fc3_b\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-30T15:31:29.998918Z",
     "start_time": "2018-03-30T15:31:29.041Z"
    }
   },
   "outputs": [],
   "source": [
    "## super-parameters\n",
    "rate = 0.001\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "## TF logic for optimization \n",
    "x = tf.placeholder(tf.float32, (None, 32, 32, 3))\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "one_hot_y = tf.one_hot(y, 43)\n",
    "\n",
    "logits = LeNet(x)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y, logits=logits)\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
    "training_operation = optimizer.minimize(loss_operation)\n",
    "\n",
    "## TF logic for evaluation\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "def evaluate(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples\n",
    "\n",
    "## training/evaluation\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = len(X_train)\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    print()\n",
    "    for i in range(EPOCHS):\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "            \n",
    "        validation_accuracy = evaluate(X_valid, y_valid)\n",
    "        print(\"EPOCH {} ...\".format(i+1))\n",
    "        print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "        print()\n",
    "        \n",
    "    saver.save(sess, './lenet')\n",
    "    print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-30T15:31:29.999699Z",
     "start_time": "2018-03-30T15:31:29.042Z"
    }
   },
   "outputs": [],
   "source": [
    "## prediction on test\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
    "\n",
    "    test_accuracy = evaluate(X_test, y_test)\n",
    "    print(\"Test Accuracy = {:.3f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-30T15:31:30.000734Z",
     "start_time": "2018-03-30T15:31:29.043Z"
    }
   },
   "outputs": [],
   "source": [
    "## analysis with top-k softmax probabilities\n",
    "import sys,os\n",
    "\n",
    "softmax = tf.nn.softmax(logits)\n",
    "\n",
    "k = 5\n",
    "num_examples = len(X_test)\n",
    "print(num_examples)\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x = X_test[offset:offset+BATCH_SIZE]\n",
    "        if(offset == 0):\n",
    "            pred_y = sess.run(softmax, feed_dict={x: batch_x})\n",
    "        else:\n",
    "            pred_y = np.vstack([pred_y, sess.run(softmax, feed_dict={x: batch_x})])\n",
    "    topk = sess.run(tf.nn.top_k(tf.constant(pred_y), k= k))\n",
    "    \n",
    "    ## top-k softmax value\n",
    "    print('softmax probabilities for top-%s' % k)\n",
    "    print(topk.values[:10])\n",
    "    \n",
    "    flattened_topk = topk.indices.reshape(1, -1).tolist()[0]\n",
    "    pred_dist = dict((i, 0.0) for i in range(43))\n",
    "    for v in flattened_topk:\n",
    "        pred_dist[v] += (1.0/(k * num_examples))\n",
    "    encoded_y = sess.run(one_hot_y, feed_dict= {y: y_test})\n",
    "    y_dist = (1.0 * np.sum(encoded_y, axis= 0))/num_examples\n",
    "    print('\\nDistribution for top-%s: ' % k)\n",
    "    print([round(pred_dist[k], 4) for k in pred_dist])\n",
    "    \n",
    "    print('\\nDistribution for truth: ')\n",
    "    print([round(v, 4) for v in y_dist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-30T15:31:30.002416Z",
     "start_time": "2018-03-30T15:31:29.044Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot(img):\n",
    "    plt.figure(figsize=(1,1))\n",
    "    plt.imshow(img, cmap=\"gray\")\n",
    "    plt.show()\n",
    "\n",
    "## analysis with top-k lowest recall for individual labels\n",
    "one_hot_pred = tf.one_hot(tf.argmax(softmax, 1), 43)\n",
    "# y_pred_equal = tf.equal(one_hot_y, one_hot_pred)\n",
    "y_pred_mult = tf.multiply(one_hot_y, one_hot_pred)\n",
    "tp = tf.reduce_sum(y_pred_mult, 0)\n",
    "t_n = tf.reduce_sum(one_hot_y, 0)\n",
    "\n",
    "# collect top-k labels with lowest recall\n",
    "k = 10\n",
    "num_examples = len(X_test)\n",
    "print('Total size for test %s ' % num_examples)\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x = X_test[offset: offset + BATCH_SIZE]\n",
    "        batch_y = y_test[offset: offset + BATCH_SIZE]\n",
    "        if(offset == 0):\n",
    "            tp_mat = sess.run(tp, feed_dict={x: batch_x, y: batch_y})\n",
    "            t_n_mat = sess.run(t_n, feed_dict={x: batch_x, y: batch_y})\n",
    "        else:\n",
    "            tp_mat = np.vstack([tp_mat, sess.run(tp, feed_dict={x: batch_x, y: batch_y})])\n",
    "            t_n_mat = np.vstack([t_n_mat, sess.run(t_n, feed_dict={x: batch_x, y: batch_y})])\n",
    "    individual_recall = np.sum(tp_mat, 0) / np.sum(t_n_mat, 0)\n",
    "    print('Recall for labels with the top-%s lowest recall: ' % k)\n",
    "    topk_recall = np.sort(individual_recall[:k])\n",
    "    print(topk_recall)\n",
    "    print('Labels with the top-%s lowest recall: ' % k)\n",
    "    topk_label = np.argsort(individual_recall)[:k]\n",
    "    print(topk_label)   \n",
    "# visualize some of these misclassified image\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
    "    \n",
    "    pred_mat = sess.run(one_hot_pred, feed_dict= {x: X_test})\n",
    "    y_mat = sess.run(one_hot_y, feed_dict={y: y_test})\n",
    "    \n",
    "    y_pred_mult_mat = np.multiply(pred_mat, y_mat)\n",
    "    # for label 27\n",
    "    debug_label = 30\n",
    "    print('\\nDebug for label %s: ' % debug_label)\n",
    "    pos_idx = np.argwhere(np.transpose(y_mat[:, debug_label]) == 1).flatten()\n",
    "    pred_pos_idx = np.argwhere(np.transpose(pred_mat[:, debug_label]) == 1).flatten()\n",
    "    \n",
    "    print(pos_idx, pred_pos_idx)\n",
    "    mis_pos_idx = [v for v in pos_idx if(v not in pred_pos_idx)]\n",
    "    print('\\nmisclassified index for label %s, size %s: ' % (debug_label, len(mis_pos_idx)))\n",
    "    print(mis_pos_idx)\n",
    "    mis_pred_label = np.argmax(pred_mat[mis_pos_idx,:], 1)\n",
    "    print('\\npredicted label for label %s: ' % debug_label)\n",
    "    print(mis_pred_label)\n",
    "    # sample the last 10 misclassified image, visualize them, see what characters they reside\n",
    "    s = 5\n",
    "    for i in range(s):\n",
    "        img = X_test[mis_pos_idx[-(s - i)]].squeeze()\n",
    "        plot(img)\n",
    "        print('predicted label %s' % mis_pred_label[- (s - i)])\n",
    "        \n",
    "## simple summarization\n",
    "# 1. nosing pixles, eg. misclassified as 25 for label 27\n",
    "# 2. darkness, eg. misclassified as other labels for label 27\n",
    "# 3. view distance\n",
    "## improvement strategies\n",
    "# 1. clips for image might help in some way\n",
    "# 2. enlighten for image might be helpful\n",
    "# 3. zoomming for image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
