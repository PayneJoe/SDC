{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-30T07:37:20.386958Z",
     "start_time": "2018-03-30T07:37:20.379311Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "import sys,os,time\n",
    "import tensorflow as tf\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-30T07:37:20.562859Z",
     "start_time": "2018-03-30T07:37:20.388636Z"
    }
   },
   "outputs": [],
   "source": [
    "## Load pickled data\n",
    "training_file = 'train.p'\n",
    "validation_file= 'valid.p'\n",
    "testing_file = 'test.p'\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(validation_file, mode='rb') as f:\n",
    "    valid = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "X_train, y_train = train['features'], train['labels']\n",
    "X_valid, y_valid = valid['features'], valid['labels']\n",
    "X_test, y_test = test['features'], test['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-30T07:37:20.688477Z",
     "start_time": "2018-03-30T07:37:20.565577Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Propotion of classes in training examples: \n",
      "[ 0.00517256  0.05689819  0.05776028  0.03620794  0.05086353  0.04741516\n",
      "  0.01034512  0.03707003  0.03620794  0.03793212  0.05172562  0.03362166\n",
      "  0.05431191  0.055174    0.01982816  0.01551769  0.01034512  0.02844909\n",
      "  0.03103537  0.00517256  0.00862094  0.00775884  0.00948303  0.01293141\n",
      "  0.00689675  0.03879422  0.01551769  0.00603466  0.0137935   0.00689675\n",
      "  0.01120722  0.01982816  0.00603466  0.01721314  0.01034512  0.03103537\n",
      "  0.00948303  0.00517256  0.05344981  0.00775884  0.00862094  0.00603466\n",
      "  0.00603466]\n",
      "Number of training examples = 34799\n",
      "Number of validation examples = 4410\n",
      "Number of testing examples = 12630\n",
      "Image data shape = (32, 32, 3)\n",
      "Number of classes = 43\n"
     ]
    }
   ],
   "source": [
    "## basic info of data sets\n",
    "# Number of training examples\n",
    "n_train = X_train.shape[0]\n",
    "\n",
    "# Number of validation examples\n",
    "n_validation = X_valid.shape[0]\n",
    "\n",
    "# Number of testing examples.\n",
    "n_test = X_test.shape[0]\n",
    "\n",
    "# What's the shape of an traffic sign image?\n",
    "image_shape = X_train[0].shape\n",
    "\n",
    "# How many unique classes/labels there are in the dataset.\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "# What propotions do these classes/labels take?\n",
    "p_classes = dict((c, 0) for c in range(n_classes))\n",
    "for l in y_train:\n",
    "    p_classes[l] += 1\n",
    "print('Propotion of classes in training examples: ')\n",
    "print(np.divide(list(p_classes.values()),np.sum(list(p_classes.values()))))\n",
    "\n",
    "print(\"Number of training examples =\", n_train)\n",
    "print(\"Number of validation examples =\", n_validation)\n",
    "print(\"Number of testing examples =\", n_test)\n",
    "print(\"Image data shape =\", image_shape)\n",
    "print(\"Number of classes =\", n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-30T07:37:20.381Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original shape: \n",
      "(34799, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "## visualization for image\n",
    "def plot(image):\n",
    "    plt.figure(figsize=(1,1))\n",
    "    plt.imshow(image, cmap=\"gray\")\n",
    "    plt.show()\n",
    "## brightness for image\n",
    "def brightness(image):\n",
    "    image = tf.image.random_hue(image, max_delta=0.05)\n",
    "    image = tf.image.random_contrast(image, lower=0.3, upper=1.0)\n",
    "    image = tf.image.random_brightness(image, max_delta=0.2)\n",
    "    image = tf.image.random_saturation(image, lower=0.0, upper=2.0)\n",
    "    \n",
    "    image = tf.cast(image, tf.uint8)\n",
    "    return image\n",
    "## noising for image\n",
    "def gauss_noise(image):\n",
    "    noise = tf.random_normal(shape=tf.shape(image), mean=0.0, stddev= 0.1, dtype=tf.float32) \n",
    "    image = tf.add(tf.cast(image, tf.float32), noise)\n",
    "    image = tf.cast(image, tf.uint8)\n",
    "    \n",
    "    return image\n",
    "\n",
    "## augmentation for train images\n",
    "def augment_data(dataset, dataset_labels, augementation_factor= 10):\n",
    "    augmented_image = []\n",
    "    augmented_image_labels = []\n",
    "\n",
    "    img_input = tf.placeholder(tf.uint8, (32, 32, 3))\n",
    "    # random brightness\n",
    "    random_brightness = brightness(img_input)\n",
    "    # random noise\n",
    "    random_noise = gauss_noise(img_input)\n",
    "    \n",
    "    sess = tf.InteractiveSession()\n",
    "    \n",
    "    for num in range (0, dataset.shape[0]):\n",
    "        # original image\n",
    "        augmented_image.append(dataset[num].tolist())\n",
    "        augmented_image_labels.append(dataset_labels[num])\n",
    "        for i in range(0, augementation_factor):\n",
    "            # brightness for image\n",
    "            result_img = sess.run(random_brightness, feed_dict= {img_input: dataset[num]})\n",
    "            augmented_image.append(result_img.tolist())\n",
    "            augmented_image_labels.append(dataset_labels[num])\n",
    "            # nosing for image\n",
    "            result_img = sess.run(random_noise, feed_dict= {img_input: dataset[num]})\n",
    "            augmented_image.append(result_img.tolist())\n",
    "            augmented_image_labels.append(dataset_labels[num])\n",
    "\n",
    "    return np.array(augmented_image), np.array(augmented_image_labels)\n",
    "\n",
    "print('original shape: ')\n",
    "print(X_train.shape)\n",
    "X_train, y_train = augment_data(X_train, y_train)\n",
    "print('augmented shape: ')\n",
    "print(X_train.shape)\n",
    "# tmp_x = X_train[:1,]\n",
    "# tmp_y = y_train[:1,]\n",
    "# print(tmp_x.shape)\n",
    "# tmp_x, tmp_y = augment_data(tmp_x, tmp_y)\n",
    "# print(tmp_x.shape)\n",
    "# for i in range(len(tmp_x)):\n",
    "#     plot(tmp_x[i])\n",
    "# sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-30T07:37:20.382Z"
    }
   },
   "outputs": [],
   "source": [
    "## visualization for image\n",
    "\n",
    "index = random.randint(0, len(X_train))\n",
    "image = X_train[index].squeeze()\n",
    "\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.imshow(image, cmap=\"gray\")\n",
    "print(y_train[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-30T07:37:20.383Z"
    }
   },
   "outputs": [],
   "source": [
    "## LE-Net architechure\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import flatten\n",
    "\n",
    "def LeNet(x):    \n",
    "    # Arguments used for tf.truncated_normal, randomly defines variables for the weights and biases for each layer\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    \n",
    "    # SOLUTION: Layer 1: Convolutional. Input = 32x32x3. \n",
    "    # out_height = (32 - 5 + 1)/1 = 28,\n",
    "    # out_width = (32 - 5 + 1)/1 = 28,\n",
    "    # out_depth = 6(tunned)\n",
    "    # Output = 28 * 28 * 6\n",
    "    conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 3, 6), mean = mu, stddev = sigma))\n",
    "    conv1_b = tf.Variable(tf.zeros(6))\n",
    "    conv1   = tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b\n",
    "\n",
    "    # SOLUTION: Activation.\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "\n",
    "    # SOLUTION: Pooling. Input = 28x28x6. \n",
    "    # out_height = (28 - 2 + 2)/2 = 14,\n",
    "    # out_width = (28 - 2 + 2)/2 = 14,\n",
    "    # out_depth = 6(handed by conv1)\n",
    "    # Output = 14x14x6.\n",
    "    conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    # SOLUTION: Layer 2: Convolutional. \n",
    "    # out_height = (14 - 5 + 1)/1 = 10,\n",
    "    # out_width = (14 - 5 + 1)/1 = 10,\n",
    "    # out_depth = 16(tunned)\n",
    "    # Output = 10x10x16.\n",
    "    conv2_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 6, 16), mean = mu, stddev = sigma))\n",
    "    conv2_b = tf.Variable(tf.zeros(16))\n",
    "    conv2   = tf.nn.conv2d(conv1, conv2_W, strides=[1, 1, 1, 1], padding='VALID') + conv2_b\n",
    "    \n",
    "    # SOLUTION: Activation.\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "\n",
    "    # SOLUTION: Pooling. Input = 10x10x16. \n",
    "    # out_height = (10 - 2 + 2)/2 = 5,\n",
    "    # out_width = (10 - 2 + 2)/2 = 5,\n",
    "    # out_depth = 16(handed by conv2)\n",
    "    # Output = 5x5x16.\n",
    "    conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    # SOLUTION: Flatten. Input = 5x5x16.\n",
    "    # Output = 5 * 5 * 16 = 400.\n",
    "    fc0   = flatten(conv2)\n",
    "    \n",
    "    # SOLUTION: Layer 3: Fully Connected. Input = 400. Output = 120.\n",
    "    fc1_W = tf.Variable(tf.truncated_normal(shape=(400, 120), mean = mu, stddev = sigma))\n",
    "    fc1_b = tf.Variable(tf.zeros(120))\n",
    "    fc1   = tf.matmul(fc0, fc1_W) + fc1_b\n",
    "    \n",
    "    # SOLUTION: Activation.\n",
    "    fc1    = tf.nn.relu(fc1)\n",
    "\n",
    "    # SOLUTION: Layer 4: Fully Connected. Input = 120. Output = 84.\n",
    "    fc2_W  = tf.Variable(tf.truncated_normal(shape=(120, 84), mean = mu, stddev = sigma))\n",
    "    fc2_b  = tf.Variable(tf.zeros(84))\n",
    "    fc2    = tf.matmul(fc1, fc2_W) + fc2_b\n",
    "    \n",
    "    # SOLUTION: Activation.\n",
    "    fc2    = tf.nn.relu(fc2)\n",
    "\n",
    "    # SOLUTION: Layer 5: Fully Connected. Input = 84. Output = 43.\n",
    "    fc3_W  = tf.Variable(tf.truncated_normal(shape=(84, 43), mean = mu, stddev = sigma))\n",
    "    fc3_b  = tf.Variable(tf.zeros(43))\n",
    "    logits = tf.matmul(fc2, fc3_W) + fc3_b\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T05:39:32.111175Z",
     "start_time": "2018-03-29T05:36:34.226370Z"
    }
   },
   "outputs": [],
   "source": [
    "## super-parameters\n",
    "rate = 0.001\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "## TF logic for optimization \n",
    "x = tf.placeholder(tf.float32, (None, 32, 32, 3))\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "one_hot_y = tf.one_hot(y, 43)\n",
    "\n",
    "logits = LeNet(x)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y, logits=logits)\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
    "training_operation = optimizer.minimize(loss_operation)\n",
    "\n",
    "## TF logic for evaluation\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "def evaluate(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples\n",
    "\n",
    "## training/evaluation\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = len(X_train)\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    print()\n",
    "    for i in range(EPOCHS):\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "            \n",
    "        validation_accuracy = evaluate(X_valid, y_valid)\n",
    "        print(\"EPOCH {} ...\".format(i+1))\n",
    "        print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "        print()\n",
    "        \n",
    "    saver.save(sess, './lenet')\n",
    "    print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-30T07:37:20.384Z"
    }
   },
   "outputs": [],
   "source": [
    "## prediction on test\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
    "\n",
    "    test_accuracy = evaluate(X_test, y_test)\n",
    "    print(\"Test Accuracy = {:.3f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-30T07:37:20.385Z"
    }
   },
   "outputs": [],
   "source": [
    "## analysis with top-k softmax probabilities\n",
    "import sys,os\n",
    "\n",
    "softmax = tf.nn.softmax(logits)\n",
    "\n",
    "k = 5\n",
    "num_examples = len(X_test)\n",
    "print(num_examples)\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x = X_test[offset:offset+BATCH_SIZE]\n",
    "        if(offset == 0):\n",
    "            pred_y = sess.run(softmax, feed_dict={x: batch_x})\n",
    "        else:\n",
    "            pred_y = np.vstack([pred_y, sess.run(softmax, feed_dict={x: batch_x})])\n",
    "    topk = sess.run(tf.nn.top_k(tf.constant(pred_y), k= k))\n",
    "    \n",
    "    ## top-k softmax value\n",
    "    print('softmax probabilities for top-%s' % k)\n",
    "    print(topk.values[:10])\n",
    "    \n",
    "    flattened_topk = topk.indices.reshape(1, -1).tolist()[0]\n",
    "    pred_dist = dict((i, 0.0) for i in range(43))\n",
    "    for v in flattened_topk:\n",
    "        pred_dist[v] += (1.0/(k * num_examples))\n",
    "    encoded_y = sess.run(one_hot_y, feed_dict= {y: y_test})\n",
    "    y_dist = (1.0 * np.sum(encoded_y, axis= 0))/num_examples\n",
    "    print('\\nDistribution for top-%s: ' % k)\n",
    "    print([round(pred_dist[k], 4) for k in pred_dist])\n",
    "    \n",
    "    print('\\nDistribution for truth: ')\n",
    "    print([round(v, 4) for v in y_dist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-30T07:37:20.386Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot(img):\n",
    "    plt.figure(figsize=(1,1))\n",
    "    plt.imshow(img, cmap=\"gray\")\n",
    "    plt.show()\n",
    "\n",
    "## analysis with top-k lowest recall for individual labels\n",
    "one_hot_pred = tf.one_hot(tf.argmax(softmax, 1), 43)\n",
    "# y_pred_equal = tf.equal(one_hot_y, one_hot_pred)\n",
    "y_pred_mult = tf.multiply(one_hot_y, one_hot_pred)\n",
    "tp = tf.reduce_sum(y_pred_mult, 0)\n",
    "t_n = tf.reduce_sum(one_hot_y, 0)\n",
    "\n",
    "# collect top-k labels with lowest recall\n",
    "k = 10\n",
    "num_examples = len(X_test)\n",
    "print('Total size for test %s ' % num_examples)\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x = X_test[offset: offset + BATCH_SIZE]\n",
    "        batch_y = y_test[offset: offset + BATCH_SIZE]\n",
    "        if(offset == 0):\n",
    "            tp_mat = sess.run(tp, feed_dict={x: batch_x, y: batch_y})\n",
    "            t_n_mat = sess.run(t_n, feed_dict={x: batch_x, y: batch_y})\n",
    "        else:\n",
    "            tp_mat = np.vstack([tp_mat, sess.run(tp, feed_dict={x: batch_x, y: batch_y})])\n",
    "            t_n_mat = np.vstack([t_n_mat, sess.run(t_n, feed_dict={x: batch_x, y: batch_y})])\n",
    "    individual_recall = np.sum(tp_mat, 0) / np.sum(t_n_mat, 0)\n",
    "    print('Recall for labels with the top-%s lowest recall: ' % k)\n",
    "    topk_recall = np.sort(individual_recall[:k])\n",
    "    print(topk_recall)\n",
    "    print('Labels with the top-%s lowest recall: ' % k)\n",
    "    topk_label = np.argsort(individual_recall)[:k]\n",
    "    print(topk_label)   \n",
    "# visualize some of these misclassified image\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
    "    \n",
    "    pred_mat = sess.run(one_hot_pred, feed_dict= {x: X_test})\n",
    "    y_mat = sess.run(one_hot_y, feed_dict={y: y_test})\n",
    "    \n",
    "    y_pred_mult_mat = np.multiply(pred_mat, y_mat)\n",
    "    # for label 27\n",
    "    debug_label = 30\n",
    "    print('\\nDebug for label %s: ' % debug_label)\n",
    "    pos_idx = np.argwhere(np.transpose(y_mat[:, debug_label]) == 1).flatten()\n",
    "    pred_pos_idx = np.argwhere(np.transpose(pred_mat[:, debug_label]) == 1).flatten()\n",
    "    \n",
    "    print(pos_idx, pred_pos_idx)\n",
    "    mis_pos_idx = [v for v in pos_idx if(v not in pred_pos_idx)]\n",
    "    print('\\nmisclassified index for label %s, size %s: ' % (debug_label, len(mis_pos_idx)))\n",
    "    print(mis_pos_idx)\n",
    "    mis_pred_label = np.argmax(pred_mat[mis_pos_idx,:], 1)\n",
    "    print('\\npredicted label for label %s: ' % debug_label)\n",
    "    print(mis_pred_label)\n",
    "    # sample the last 10 misclassified image, visualize them, see what characters they reside\n",
    "    s = 5\n",
    "    for i in range(s):\n",
    "        img = X_test[mis_pos_idx[-(s - i)]].squeeze()\n",
    "        plot(img)\n",
    "        print('predicted label %s' % mis_pred_label[- (s - i)])\n",
    "        \n",
    "## simple summarization\n",
    "# 1. nosing pixles, eg. misclassified as 25 for label 27\n",
    "# 2. darkness, eg. misclassified as other labels for label 27\n",
    "# 3. view distance\n",
    "## improvement strategies\n",
    "# 1. clips for image might help in some way\n",
    "# 2. enlighten for image might be helpful\n",
    "# 3. zoomming for image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
